{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373\n",
      "                                                                  text task1  \\\n",
      "tweet_id                                                                       \n",
      "1133388798925189122  Deutsche rothaarige porno reife deutsche fraue...   NOT   \n",
      "1127134592517980161  RT @NDRinfo: Die deutsche Klimaaktivistin Luis...   NOT   \n",
      "1128897106171842560  @ruhrbahn jeden Morgen eine neue â€žFahrzeugstÃ¶r...   NOT   \n",
      "1123576753199484928  @Junge_Freiheit Die Inkas hatten sich schon dÃ¤...   NOT   \n",
      "1128743783393312768  RT @technosteron: leute die 'boar' schreiben l...   HOF   \n",
      "\n",
      "                    task2                  ID  \n",
      "tweet_id                                       \n",
      "1133388798925189122  NONE  hasoc_2020_de_2684  \n",
      "1127134592517980161  NONE  hasoc_2020_de_1042  \n",
      "1128897106171842560  NONE   hasoc_2020_de_774  \n",
      "1123576753199484928  NONE   hasoc_2020_de_559  \n",
      "1128743783393312768  PRFN  hasoc_2020_de_1969  \n",
      "{0, 1, 2, 3}\n",
      "7042 7042 526 526\n",
      "                tweet_id                                               text  \\\n",
      "0    1129095874242650112    @derCarsti Boykottieren hÃ¶rt sich besser an. ðŸ’™ðŸ’™   \n",
      "1    1129004308396236800  RT @ibikus31: Es wird spekuliert, ob Merkel ei...   \n",
      "2    1130896929355907080  Hat #Hitler wirklich den Krieg in der WÃ¼ste ve...   \n",
      "3    1132251534329307136  RT @Beatrix_vStorch: #May tritt in UK unter Tr...   \n",
      "4    1124941869115498496  @justmeDoro Eher nicht. Das GÃ¤nse hauen wieder...   \n",
      "..                   ...                                                ...   \n",
      "521  1124809878546128897  RT @ChanMachtSo: SCHMERZEN!!!! Au!!! Mein Gehi...   \n",
      "522  1132433240000798720  Die ZerstÃ¶rung der GrÃ¼nen. https://t.co/SIYDJj...   \n",
      "523  1127366294255357958  RT @PParzival: \"Antideutsche\" Pseudo-linke Ide...   \n",
      "524  1124362090460975105                         Klug reden und dumm leben.   \n",
      "525  1131487097293103104  Wissen wir schon lange....hat das die Merkel e...   \n",
      "\n",
      "    task1 task2                  ID  \n",
      "0     NOT  NONE  hasoc_2020_de_1053  \n",
      "1     NOT  NONE   hasoc_2020_de_671  \n",
      "2     NOT  NONE  hasoc_2020_de_2977  \n",
      "3     NOT  NONE  hasoc_2020_de_1746  \n",
      "4     NOT  NONE  hasoc_2020_de_2416  \n",
      "..    ...   ...                 ...  \n",
      "521   NOT  NONE   hasoc_2020_de_486  \n",
      "522   NOT  NONE  hasoc_2020_de_3388  \n",
      "523   NOT  NONE  hasoc_2020_de_2745  \n",
      "524   NOT  NONE   hasoc_2020_de_236  \n",
      "525   NOT  NONE  hasoc_2020_de_2850  \n",
      "\n",
      "[526 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "file = \"hasoc_2020_de_train_new_b.xlsx\"\n",
    "file_test = \"german_test_1509.csv\"\n",
    "\n",
    "\n",
    "df_train = pd.read_excel(file,index_col=0)\n",
    "\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "df_test = pd.read_csv(file_test)\n",
    "\n",
    "task = 'task2'\n",
    "task_2019 = 'task_2'\n",
    "\n",
    "#2019 datasets also\n",
    "\n",
    "file_2019_1 = pd.read_csv(\"2019/german_dataset/german_dataset/hasoc_de_test_gold.tsv\",sep='\\t')\n",
    "file_2019_2 = pd.read_csv(\"2019/german_dataset/german_dataset/german_dataset.tsv\",sep=\"\\t\")\n",
    "\n",
    "\n",
    "sentences_2019_1 = list(file_2019_1['text'].values)\n",
    "sentences_2019_2 = list(file_2019_2['text'].values)\n",
    "\n",
    "labels_2019_1 = list(file_2019_1[task_2019].values)\n",
    "labels_2019_2 = list(file_2019_2[task_2019].values)\n",
    "\n",
    "print(len(df_train))\n",
    "print(df_train.head())\n",
    "\n",
    "total_sentences = list(df_train['text'].values)\n",
    "total_labels = list(df_train[task].values)\n",
    "\n",
    "total_sentences.extend(sentences_2019_1)\n",
    "total_sentences.extend(sentences_2019_2)\n",
    "\n",
    "total_labels.extend(labels_2019_1)\n",
    "total_labels.extend(labels_2019_2)\n",
    "\n",
    "test_sentences = list(df_test['text'].values)\n",
    "test_labels = list(df_test[task].values)\n",
    "\n",
    "def clean_text(sentences):\n",
    "    for index,line in enumerate(sentences):\n",
    "        if \"\\n\" in line:\n",
    "            sentences[index] = line.replace(\"\\n\",\"\")\n",
    "    return sentences\n",
    "        \n",
    "total_sentences = clean_text(total_sentences)\n",
    "test_sentences = clean_text(test_sentences)\n",
    "\n",
    "def clean_labels(labels):\n",
    "    new_list= []\n",
    "    for value in labels:\n",
    "        new_list.append(value.strip())\n",
    "    return new_list\n",
    "\n",
    "total_labels = clean_labels(total_labels)\n",
    "test_labels = clean_labels(test_labels)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(total_labels)\n",
    "encoded_labels = le.transform(total_labels)\n",
    "encoded_test_labels = le.transform(test_labels)\n",
    "print(set(encoded_labels))\n",
    "\n",
    "print(len(total_sentences),len(encoded_labels),len(test_sentences),len(encoded_test_labels))\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length is:  175\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "max_length = 0\n",
    "for sentence in total_sentences:\n",
    "    #print(sentence)\n",
    "    length = len(tokenizer.tokenize(sentence))\n",
    "    if length > max_length:\n",
    "        max_length  = length\n",
    "print(\"max token length is: \",max_length)\n",
    "# max token length obtained is 50\n",
    "# bert tokens are limited to 514 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Deutsche rothaarige porno reife deutsche frauen porno. Deutsche politessen pornos porno deutsch inzets. https://t.co/xAag87Y0Jd\n",
      "Token IDs: tensor([   101,  15389,  64354,  55200,  45854,  10183,  10343,  14243,  14601,\n",
      "         17486,  10628,  25733,  10183,  10343,    119,  15389,  91929, 100319,\n",
      "         10115,  10183,  14386,  10183,  10343,  24722,  10106,  17931,  10107,\n",
      "           119,  14120,    131,    120,    120,    188,    119,  11170,    120,\n",
      "           192,  10738,  14520,  11396,  11305,  14703,  10929,  15417,  10162,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "def encoder_generator(sentences,labels):\n",
    "    \n",
    "    sent_index = []\n",
    "    input_ids = []\n",
    "    attention_masks =[]\n",
    "\n",
    "    for index,sent in enumerate(sentences):\n",
    "        \n",
    "        sent_index.append(index)\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=max_length,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             truncation = True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids,dim=0)\n",
    "    attention_masks = torch.cat(attention_masks,dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    sent_index = torch.tensor(sent_index)\n",
    "\n",
    "    return sent_index,input_ids,attention_masks,labels\n",
    "\n",
    "sent_index,input_ids,attention_masks,encoded_label_tensors = encoder_generator(total_sentences,encoded_labels)\n",
    "test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors = encoder_generator(test_sentences,encoded_test_labels)\n",
    "print('Original: ', total_sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "#print(encoded_label_tensors)\n",
    "#print(encoded_test_label_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data samples is 5281\n",
      "valid data samples is 1761\n",
      "test data samples is 526\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset,random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids,attention_masks,encoded_label_tensors)\n",
    "test_dataset = TensorDataset(test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors)\n",
    "\n",
    "train_size = int(0.75*len(dataset))\n",
    "\n",
    "val_size = len(dataset)-train_size\n",
    "\n",
    "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\n",
    "\n",
    "print('train data samples is {}'.format(len(train_dataset)))\n",
    "print(\"valid data samples is {}\".format(len(val_dataset)))\n",
    "print(\"test data samples is {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "bs=8\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                              sampler=RandomSampler(train_dataset),\n",
    "                              batch_size=bs)\n",
    "valid_data_loader = DataLoader(val_dataset,\n",
    "                              sampler=SequentialSampler(val_dataset),\n",
    "                              batch_size=bs)\n",
    "test_data_loader = DataLoader(test_dataset,\n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',\n",
    "                                                     num_labels=len(le.classes_),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False,\n",
    "                                                     )\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs=10\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=0,\n",
    "                                           num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predictions_labels(preds,labels):\n",
    "    pred = np.argmax(preds,axis=1).flatten()\n",
    "    label = labels.flatten()\n",
    "    return pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "def predictions_labels(preds,labels):\n",
    "    pred = np.argmax(preds,axis=1).flatten()\n",
    "    label = labels.flatten()\n",
    "    return pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    \n",
    "    model.train() # set model in train mode for batchnorm and dropout layers in bert model\n",
    "    \n",
    "    for step,batch in enumerate(train_data_loader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "            \n",
    "        loss,logits = model(b_input_ids,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels.long())\n",
    "            \n",
    "        total_train_loss+=loss.item()\n",
    "        total_train_acc+=categorical_accuracy(logits,b_labels).item()\n",
    "            \n",
    "        loss.backward()\n",
    "            \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            \n",
    "        optimizer.step()\n",
    "            \n",
    "        scheduler.step() #go ahead and update the learning rate\n",
    "            \n",
    "    avg_train_loss = total_train_loss/len(train_data_loader)\n",
    "    avg_train_acc = total_train_acc/len(train_data_loader)\n",
    "    \n",
    "    return avg_train_loss,avg_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "        \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    number_of_eval_steps= 0\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for batch in valid_data_loader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                attention_mask= b_input_mask,\n",
    "                                labels = b_labels.long())\n",
    "        total_eval_loss+=loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred,true = predictions_labels(logits,label_ids)\n",
    "        \n",
    "        all_pred_labels.extend(pred)\n",
    "        all_true_labels.extend(true)\n",
    "\n",
    "    print(classification_report(all_pred_labels,all_true_labels))\n",
    "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
    "    macro_f1_score = f1_score(all_pred_labels,all_true_labels,average='macro')\n",
    "    \n",
    "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
    "\n",
    "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    return avg_val_loss,avg_val_accuracy,macro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suman\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\suman\\Miniconda3\\envs\\py3_env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.97      0.89      0.93      1602\n",
      "           2       0.10      0.61      0.18        18\n",
      "           3       0.71      0.54      0.61       141\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1761\n",
      "   macro avg       0.45      0.51      0.43      1761\n",
      "weighted avg       0.94      0.86      0.89      1761\n",
      "\n",
      "accuracy = 0.86\n",
      "Epoch: 01 | Epoch Time: 3m 20s\n",
      "\tTrain Loss: 0.606 | Train Acc: 83.23%\n",
      "\t Val. Loss: 0.538 |  Val. Acc: 85.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.50      0.03         2\n",
      "           1       0.98      0.88      0.93      1637\n",
      "           2       0.09      1.00      0.16         9\n",
      "           3       0.64      0.60      0.62       113\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1761\n",
      "   macro avg       0.43      0.75      0.43      1761\n",
      "weighted avg       0.95      0.86      0.90      1761\n",
      "\n",
      "accuracy = 0.86\n",
      "Epoch: 02 | Epoch Time: 3m 9s\n",
      "\tTrain Loss: 0.493 | Train Acc: 86.14%\n",
      "\t Val. Loss: 0.518 |  Val. Acc: 86.26%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.36      0.09        11\n",
      "           1       0.97      0.89      0.93      1613\n",
      "           2       0.18      0.49      0.26        39\n",
      "           3       0.63      0.68      0.65        98\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1761\n",
      "   macro avg       0.46      0.61      0.48      1761\n",
      "weighted avg       0.93      0.86      0.89      1761\n",
      "\n",
      "accuracy = 0.86\n",
      "Epoch: 03 | Epoch Time: 3m 10s\n",
      "\tTrain Loss: 0.399 | Train Acc: 88.62%\n",
      "\t Val. Loss: 0.615 |  Val. Acc: 86.48%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.33      0.21        36\n",
      "           1       0.97      0.89      0.93      1603\n",
      "           2       0.16      0.68      0.26        25\n",
      "           3       0.64      0.70      0.67        97\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1761\n",
      "   macro avg       0.48      0.65      0.52      1761\n",
      "weighted avg       0.92      0.87      0.89      1761\n",
      "\n",
      "accuracy = 0.87\n",
      "Epoch: 04 | Epoch Time: 3m 11s\n",
      "\tTrain Loss: 0.301 | Train Acc: 91.70%\n",
      "\t Val. Loss: 0.669 |  Val. Acc: 86.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.24      0.14        34\n",
      "           1       0.96      0.89      0.92      1576\n",
      "           2       0.27      0.42      0.33        66\n",
      "           3       0.53      0.67      0.59        85\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1761\n",
      "   macro avg       0.46      0.56      0.50      1761\n",
      "weighted avg       0.89      0.85      0.87      1761\n",
      "\n",
      "accuracy = 0.85\n",
      "Epoch: 05 | Epoch Time: 3m 10s\n",
      "\tTrain Loss: 0.201 | Train Acc: 95.12%\n",
      "\t Val. Loss: 0.832 |  Val. Acc: 85.18%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.24      0.22        68\n",
      "           1       0.93      0.90      0.92      1526\n",
      "           2       0.16      0.57      0.25        30\n",
      "           3       0.69      0.54      0.61       137\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1761\n",
      "   macro avg       0.50      0.56      0.50      1761\n",
      "weighted avg       0.87      0.84      0.85      1761\n",
      "\n",
      "accuracy = 0.84\n",
      "Epoch: 06 | Epoch Time: 3m 11s\n",
      "\tTrain Loss: 0.116 | Train Acc: 97.37%\n",
      "\t Val. Loss: 0.952 |  Val. Acc: 84.04%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.35      0.14        20\n",
      "           1       0.96      0.89      0.93      1591\n",
      "           2       0.23      0.38      0.29        63\n",
      "           3       0.57      0.70      0.63        87\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1761\n",
      "   macro avg       0.46      0.58      0.50      1761\n",
      "weighted avg       0.91      0.86      0.88      1761\n",
      "\n",
      "accuracy = 0.86\n",
      "Epoch: 07 | Epoch Time: 3m 12s\n",
      "\tTrain Loss: 0.075 | Train Acc: 98.41%\n",
      "\t Val. Loss: 1.043 |  Val. Acc: 85.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.26      0.19        47\n",
      "           1       0.95      0.89      0.92      1567\n",
      "           2       0.18      0.40      0.25        47\n",
      "           3       0.63      0.67      0.65       100\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1761\n",
      "   macro avg       0.48      0.56      0.50      1761\n",
      "weighted avg       0.89      0.85      0.87      1761\n",
      "\n",
      "accuracy = 0.85\n",
      "Epoch: 08 | Epoch Time: 3m 12s\n",
      "\tTrain Loss: 0.037 | Train Acc: 99.17%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 85.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.25      0.22        65\n",
      "           1       0.94      0.90      0.92      1549\n",
      "           2       0.19      0.48      0.27        42\n",
      "           3       0.64      0.66      0.65       105\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1761\n",
      "   macro avg       0.50      0.57      0.52      1761\n",
      "weighted avg       0.88      0.85      0.86      1761\n",
      "\n",
      "accuracy = 0.85\n",
      "Epoch: 09 | Epoch Time: 3m 11s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.68%\n",
      "\t Val. Loss: 1.196 |  Val. Acc: 84.84%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.27      0.20        49\n",
      "           1       0.95      0.89      0.92      1570\n",
      "           2       0.19      0.42      0.26        48\n",
      "           3       0.63      0.71      0.67        94\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1761\n",
      "   macro avg       0.48      0.57      0.51      1761\n",
      "weighted avg       0.89      0.85      0.87      1761\n",
      "\n",
      "accuracy = 0.85\n",
      "Epoch: 10 | Epoch Time: 3m 9s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.218 |  Val. Acc: 85.41%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "best_macro_f1 = float('0')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss,train_acc = train()\n",
    "    valid_loss,valid_acc,macro_f1 = evaluate()\n",
    "    \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model,'model_german_task_b.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "model = torch.load('model_german_task_b.pt')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.36      0.23        11\n",
      "           1       0.93      0.86      0.89       407\n",
      "           2       0.06      0.40      0.10         5\n",
      "           3       0.75      0.64      0.69       103\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       526\n",
      "   macro avg       0.48      0.57      0.48       526\n",
      "weighted avg       0.87      0.80      0.83       526\n",
      "\n",
      "accuracy = 0.80\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test():\n",
    "    model.eval()\n",
    "        \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    number_of_eval_steps= 0\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    all_sentence_id=[]\n",
    "\n",
    "    for batch in test_data_loader:\n",
    "        b_sentence_id = batch[0].to(device)\n",
    "        b_input_ids = batch[1].to(device)\n",
    "        b_input_mask = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        sent_ids = b_sentence_id.to('cpu').numpy()\n",
    "        all_sentence_id.extend(sent_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                attention_mask= b_input_mask,\n",
    "                                labels = b_labels.long())\n",
    "        total_eval_loss+=loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "\n",
    "        pred,true = predictions_labels(logits,label_ids)\n",
    "        \n",
    "        all_pred_labels.extend(pred)\n",
    "        \n",
    "        all_true_labels.extend(true)\n",
    "\n",
    "    print(classification_report(all_pred_labels,all_true_labels))\n",
    "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
    "    \n",
    "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
    "\n",
    "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    return avg_val_loss,avg_val_accuracy,all_sentence_id,all_pred_labels\n",
    "\n",
    "valid_loss,valid_acc,all_sentence_id,all_pred_labels = evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = list(le.inverse_transform(all_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "dict_index_y_value = {}\n",
    "\n",
    "for index,y_value in zip(all_sentence_id,pred_labels):\n",
    "    dict_index_y_value[index] = y_value\n",
    "\n",
    "od = OrderedDict(sorted(dict_index_y_value.items()))\n",
    "\n",
    "sorted_y_predicts = []\n",
    "for k,v in od.items():\n",
    "    sorted_y_predicts.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet_id = list(df_test['tweet_id'].values)\n",
    "test_ID = list(df_test['ID'].values)\n",
    "\n",
    "test_final = pd.DataFrame(zip(test_tweet_id,sorted_y_predicts,test_ID),columns=['tweet_id',task,'ID'])\n",
    "\n",
    "if task == 'task1':\n",
    "    test_final.to_csv(\"submission_DE_A.csv\",index=False)\n",
    "\n",
    "if task == 'task2':\n",
    "    test_final.to_csv(\"submission_DE_B.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3_env)",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
