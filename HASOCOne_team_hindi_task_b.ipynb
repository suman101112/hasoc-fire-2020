{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2963\n",
      "                                                                  text task1  \\\n",
      "tweet_id                                                                       \n",
      "1127755185944711170  1 à¤†à¤¦à¤®à¥€à¤‚ à¤•à¥‹ à¤®à¤¾à¤°à¤¨à¥‡ à¤ªà¤° à¤—à¥‹à¤¡à¤¸à¥‡ à¤†à¤¤à¤‚à¤•à¥€ à¤¹à¥‹ à¤¸à¤•à¥‡ à¤¹à¥ˆ à¤¤à¥‹\\n...   HOF   \n",
      "1123578938406592513  RT @Vishesh4: @jawaharyadavbjp à¤œà¤µà¤¾à¤¹à¤° à¤¯à¤¾à¤¦à¤µ, à¤…à¤—à¤°...   NOT   \n",
      "1127750010156642304  RT @FunKeyBaat: #à¤­à¤—à¤µà¤¾ à¤µà¤¸à¥à¤¤à¥à¤° à¤ªà¤¹à¤¨ à¤•à¤° à¤®à¤¤à¤¦à¤¾à¤¨ à¤¨à¤¹à¥€ ...   HOF   \n",
      "1127660759553597441  Yey nina khothani labafazi benu phambili Finis...   HOF   \n",
      "1123487255136489472  RT @Rajeshbhanjan2: à¤œà¤¬ à¤­à¥€ à¤•à¥‹à¤ˆ à¤¸à¤¿à¤•à¥à¤²à¤° à¤•à¥‹à¤‚à¤—à¥à¤°à¥‡à¤¸à¥€...   HOF   \n",
      "\n",
      "                    task2                  ID  \n",
      "tweet_id                                       \n",
      "1127755185944711170  HATE  hasoc_2020_hi_2250  \n",
      "1123578938406592513  NONE   hasoc_2020_hi_381  \n",
      "1127750010156642304  HATE  hasoc_2020_hi_1510  \n",
      "1127660759553597441  PRFN  hasoc_2020_hi_1010  \n",
      "1123487255136489472  HATE   hasoc_2020_hi_331  \n",
      "{0, 1, 2, 3}\n",
      "8946 8946 663 663\n",
      "                tweet_id                                               text  \\\n",
      "0    1127781647783301120  @Kinjal_Dubey_ à¤‡à¤¸à¤•à¥‡ à¤²à¤¿à¤ à¤¤à¥ˆà¤¯à¤¾à¤°à¥€ à¤¹à¥‹ à¤°à¤–à¥€ à¤¹à¥ˆ\\nà¤†à¤ªà¤•à¥‹...   \n",
      "1    1127492853171535872  RT @pratimamishra04: à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤®à¥à¤–à¥à¤¯à¤®à¤‚à¤¤à¥à¤°à¥€ @Ar...   \n",
      "2    1123638375896903681  RT @nishantdabre2: à¤†à¤œ à¤à¤• à¤¤à¤°à¤« à¤–à¥à¤¶à¥€ à¤¹à¥ˆ, à¤¤à¥‹ à¤à¤• à¤¤à¤°...   \n",
      "3    1127496460310683648  @BJP4India @HardeepSPuri à¤œà¤¿à¤¸ à¤•à¥à¤¨à¤¬à¥‡ à¤•à¥‡ à¤²à¥‹à¤— :-\\n...   \n",
      "4    1127667277535399937                            @manakgupta à¤¬à¤¾à¤¦à¤²....ðŸ˜‚ðŸ˜‚ðŸ˜‚   \n",
      "..                   ...                                                ...   \n",
      "658  1123814066907242503  RT @AishwaryVerma9: à¤¦à¤¿à¤²à¥à¤²à¥€ à¤®à¥‡à¤‚ à¤¶à¥‚à¤¨à¥à¤¯ à¤¥à¥‡ à¤”à¤° à¤¶à¥‚à¤¨...   \n",
      "659  1127616035689979904  RT @KARUNASHANKEROJ: @sagarikaghose à¤à¤• à¤†à¤°à¥à¤Ÿà¤¿à¤•à¤²...   \n",
      "660  1127502823044702208  @CM_P17 @PShatrujeet à¤°à¤®à¤œà¤¾à¤¨ à¤¶à¥à¤°à¥‚ à¤²à¥‡à¤•à¤¿à¤¨ à¤®à¤œà¤¾à¤² à¤¹à¥ˆ ...   \n",
      "661  1127780829914992640  RT @rukasa007: à¤¤à¥‹ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥‡ à¤¸à¤¿à¤° à¤®à¥‡à¤‚ à¤à¤• à¤—à¥‹à¤²à¥€ à¤‰à¤¤à¤¾...   \n",
      "662  1127465619580788737  @athavale_abhi @kavita_tewari à¤—à¤¾à¤‚à¤§à¥€ à¤ªà¤°à¤¿à¤µà¤¾à¤° à¤”à¤° ...   \n",
      "\n",
      "    task1 task2                  ID  \n",
      "0     HOF  OFFN  hasoc_2020_hi_1938  \n",
      "1     NOT  NONE  hasoc_2020_hi_1201  \n",
      "2     NOT  NONE  hasoc_2020_hi_1628  \n",
      "3     NOT  NONE  hasoc_2020_hi_3257  \n",
      "4     NOT  NONE  hasoc_2020_hi_4133  \n",
      "..    ...   ...                 ...  \n",
      "658   NOT  NONE   hasoc_2020_hi_466  \n",
      "659   HOF  NONE  hasoc_2020_hi_1758  \n",
      "660   HOF  OFFN  hasoc_2020_hi_3521  \n",
      "661   NOT  NONE  hasoc_2020_hi_1293  \n",
      "662   HOF  HATE  hasoc_2020_hi_3279  \n",
      "\n",
      "[663 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "file = \"hasoc_2020_hi_train_b.xlsx\"\n",
    "file_test = \"hindi_test_1509.csv\"\n",
    "\n",
    "\n",
    "df_train = pd.read_excel(file,index_col=0)\n",
    "\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "df_test = pd.read_csv(file_test)\n",
    "\n",
    "task = 'task2'\n",
    "task_2019 = 'task_2'\n",
    "\n",
    "#2019 datasets also\n",
    "\n",
    "file_2019_1 = pd.read_csv(\"2019/hindi_dataset/hindi_dataset/hasoc2019_hi_test_gold_2919.tsv\",sep='\\t')\n",
    "file_2019_2 = pd.read_csv(\"2019/hindi_dataset/hindi_dataset/hindi_dataset.tsv\",sep=\"\\t\")\n",
    "\n",
    "\n",
    "sentences_2019_1 = list(file_2019_1['text'].values)\n",
    "sentences_2019_2 = list(file_2019_2['text'].values)\n",
    "\n",
    "labels_2019_1 = list(file_2019_1[task_2019].values)\n",
    "labels_2019_2 = list(file_2019_2[task_2019].values)\n",
    "\n",
    "print(len(df_train))\n",
    "print(df_train.head())\n",
    "\n",
    "total_sentences = list(df_train['text'].values)\n",
    "total_labels = list(df_train[task].values)\n",
    "\n",
    "total_sentences.extend(sentences_2019_1)\n",
    "total_sentences.extend(sentences_2019_2)\n",
    "\n",
    "total_labels.extend(labels_2019_1)\n",
    "total_labels.extend(labels_2019_2)\n",
    "\n",
    "test_sentences = list(df_test['text'].values)\n",
    "test_labels = list(df_test[task].values)\n",
    "\n",
    "def clean_text(sentences):\n",
    "    for index,line in enumerate(sentences):\n",
    "        if \"\\n\" in line:\n",
    "            sentences[index] = line.replace(\"\\n\",\"\")\n",
    "    return sentences\n",
    "        \n",
    "total_sentences = clean_text(total_sentences)\n",
    "test_sentences = clean_text(test_sentences)\n",
    "\n",
    "def clean_labels(labels):\n",
    "    new_list= []\n",
    "    for value in labels:\n",
    "        new_list.append(value.strip())\n",
    "    return new_list\n",
    "\n",
    "total_labels = clean_labels(total_labels)\n",
    "test_labels = clean_labels(test_labels)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(total_labels)\n",
    "encoded_labels = le.transform(total_labels)\n",
    "encoded_test_labels = le.transform(test_labels)\n",
    "print(set(encoded_labels))\n",
    "\n",
    "print(len(total_sentences),len(encoded_labels),len(test_sentences),len(encoded_test_labels))\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length is:  297\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "max_length = 0\n",
    "for sentence in total_sentences:\n",
    "    #print(sentence)\n",
    "    length = len(tokenizer.tokenize(sentence))\n",
    "    if length > max_length:\n",
    "        max_length  = length\n",
    "print(\"max token length is: \",max_length)\n",
    "# max token length obtained is 50\n",
    "# bert tokens are limited to 514 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  1 à¤†à¤¦à¤®à¥€à¤‚ à¤•à¥‹ à¤®à¤¾à¤°à¤¨à¥‡ à¤ªà¤° à¤—à¥‹à¤¡à¤¸à¥‡ à¤†à¤¤à¤‚à¤•à¥€ à¤¹à¥‹ à¤¸à¤•à¥‡ à¤¹à¥ˆ à¤¤à¥‹17000 à¤¸à¤¿à¤–à¥‹, 5000 à¤­à¥‹à¤ªà¤¾à¤²à¥€, 3000 à¤¤à¤®à¤¿à¤²à¥‹à¤‚ à¤•à¤¾ à¤•à¤¤à¥à¤²à¥‡à¤†à¤® à¤•à¤°à¤µà¤¾à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤­à¤¾à¤°à¤¤ à¤°à¤¤à¥à¤¨ à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹ à¤¸à¤•à¥‡ à¤¹à¥ˆ?\n",
      "Token IDs: tensor([   101,    122,    852,  15552,  40340,  14018,  11267,  32629,  11549,\n",
      "         13466,  12213,    867,  69334,  35622,    852,  11845,  90696,  10914,\n",
      "         13220,    898,  38150,  10569,  21042,  34264,  77802,    898,  12878,\n",
      "         27841,  13718,    117,  17436,    888,  65430,  88113,    117,  15335,\n",
      "         37843,  11497,  11081,    865,  11845,  50101,  11554, 111193,  13841,\n",
      "         16192,  28960,  13466,  62332,  14311,    891,  11845,  22949,    865,\n",
      "         18438,  35622,  13220,    898,  38150,  10569,    136,    102,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "def encoder_generator(sentences,labels):\n",
    "    \n",
    "    sent_index = []\n",
    "    input_ids = []\n",
    "    attention_masks =[]\n",
    "\n",
    "    for index,sent in enumerate(sentences):\n",
    "        \n",
    "        sent_index.append(index)\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=max_length,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             truncation = True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids,dim=0)\n",
    "    attention_masks = torch.cat(attention_masks,dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    sent_index = torch.tensor(sent_index)\n",
    "\n",
    "    return sent_index,input_ids,attention_masks,labels\n",
    "\n",
    "sent_index,input_ids,attention_masks,encoded_label_tensors = encoder_generator(total_sentences,encoded_labels)\n",
    "test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors = encoder_generator(test_sentences,encoded_test_labels)\n",
    "print('Original: ', total_sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "#print(encoded_label_tensors)\n",
    "#print(encoded_test_label_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data samples is 6709\n",
      "valid data samples is 2237\n",
      "test data samples is 663\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset,random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids,attention_masks,encoded_label_tensors)\n",
    "test_dataset = TensorDataset(test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors)\n",
    "\n",
    "train_size = int(0.75*len(dataset))\n",
    "\n",
    "val_size = len(dataset)-train_size\n",
    "\n",
    "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\n",
    "\n",
    "print('train data samples is {}'.format(len(train_dataset)))\n",
    "print(\"valid data samples is {}\".format(len(val_dataset)))\n",
    "print(\"test data samples is {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "bs=8\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                              sampler=RandomSampler(train_dataset),\n",
    "                              batch_size=bs)\n",
    "valid_data_loader = DataLoader(val_dataset,\n",
    "                              sampler=SequentialSampler(val_dataset),\n",
    "                              batch_size=bs)\n",
    "test_data_loader = DataLoader(test_dataset,\n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',\n",
    "                                                     num_labels=len(le.classes_),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False,\n",
    "                                                     )\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs=10\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=0,\n",
    "                                           num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predictions_labels(preds,labels):\n",
    "    pred = np.argmax(preds,axis=1).flatten()\n",
    "    label = labels.flatten()\n",
    "    return pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "def predictions_labels(preds,labels):\n",
    "    pred = np.argmax(preds,axis=1).flatten()\n",
    "    label = labels.flatten()\n",
    "    return pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    \n",
    "    model.train() # set model in train mode for batchnorm and dropout layers in bert model\n",
    "    \n",
    "    for step,batch in enumerate(train_data_loader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "            \n",
    "        loss,logits = model(b_input_ids,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels.long())\n",
    "            \n",
    "        total_train_loss+=loss.item()\n",
    "        total_train_acc+=categorical_accuracy(logits,b_labels).item()\n",
    "            \n",
    "        loss.backward()\n",
    "            \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            \n",
    "        optimizer.step()\n",
    "            \n",
    "        scheduler.step() #go ahead and update the learning rate\n",
    "            \n",
    "    avg_train_loss = total_train_loss/len(train_data_loader)\n",
    "    avg_train_acc = total_train_acc/len(train_data_loader)\n",
    "    \n",
    "    return avg_train_loss,avg_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "        \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    number_of_eval_steps= 0\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for batch in valid_data_loader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                attention_mask= b_input_mask,\n",
    "                                labels = b_labels.long())\n",
    "        total_eval_loss+=loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred,true = predictions_labels(logits,label_ids)\n",
    "        \n",
    "        all_pred_labels.extend(pred)\n",
    "        all_true_labels.extend(true)\n",
    "\n",
    "    print(classification_report(all_pred_labels,all_true_labels))\n",
    "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
    "    macro_f1_score = f1_score(all_pred_labels,all_true_labels,average='macro')\n",
    "    \n",
    "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
    "\n",
    "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    return avg_val_loss,avg_val_accuracy,macro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.91      0.74      0.82      1536\n",
      "           2       0.22      0.38      0.28       203\n",
      "           3       0.83      0.65      0.73       496\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2237\n",
      "   macro avg       0.49      0.44      0.45      2237\n",
      "weighted avg       0.83      0.69      0.75      2237\n",
      "\n",
      "accuracy = 0.69\n",
      "Epoch: 01 | Epoch Time: 5m 44s\n",
      "\tTrain Loss: 0.929 | Train Acc: 67.12%\n",
      "\t Val. Loss: 0.892 |  Val. Acc: 68.53%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.30      0.31       271\n",
      "           1       0.91      0.76      0.83      1497\n",
      "           2       0.19      0.48      0.27       139\n",
      "           3       0.73      0.85      0.78       330\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      2237\n",
      "   macro avg       0.54      0.60      0.55      2237\n",
      "weighted avg       0.77      0.70      0.73      2237\n",
      "\n",
      "accuracy = 0.70\n",
      "Epoch: 02 | Epoch Time: 5m 46s\n",
      "\tTrain Loss: 0.745 | Train Acc: 72.96%\n",
      "\t Val. Loss: 0.794 |  Val. Acc: 70.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.39      0.18        71\n",
      "           1       0.94      0.75      0.83      1563\n",
      "           2       0.28      0.46      0.35       217\n",
      "           3       0.79      0.79      0.79       386\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      2237\n",
      "   macro avg       0.53      0.60      0.54      2237\n",
      "weighted avg       0.82      0.71      0.76      2237\n",
      "\n",
      "accuracy = 0.71\n",
      "Epoch: 03 | Epoch Time: 5m 45s\n",
      "\tTrain Loss: 0.638 | Train Acc: 76.91%\n",
      "\t Val. Loss: 0.984 |  Val. Acc: 71.48%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.36      0.29       169\n",
      "           1       0.82      0.81      0.81      1262\n",
      "           2       0.47      0.37      0.42       456\n",
      "           3       0.75      0.83      0.79       350\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2237\n",
      "   macro avg       0.57      0.59      0.58      2237\n",
      "weighted avg       0.69      0.69      0.69      2237\n",
      "\n",
      "accuracy = 0.69\n",
      "Epoch: 04 | Epoch Time: 5m 45s\n",
      "\tTrain Loss: 0.516 | Train Acc: 81.74%\n",
      "\t Val. Loss: 0.931 |  Val. Acc: 68.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.25      0.30       404\n",
      "           1       0.76      0.81      0.78      1173\n",
      "           2       0.28      0.37      0.32       267\n",
      "           3       0.78      0.77      0.78       393\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      2237\n",
      "   macro avg       0.56      0.55      0.55      2237\n",
      "weighted avg       0.64      0.65      0.64      2237\n",
      "\n",
      "accuracy = 0.65\n",
      "Epoch: 05 | Epoch Time: 5m 45s\n",
      "\tTrain Loss: 0.410 | Train Acc: 86.08%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 64.82%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.34      0.27       165\n",
      "           1       0.87      0.77      0.82      1412\n",
      "           2       0.35      0.42      0.38       295\n",
      "           3       0.75      0.79      0.77       365\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      2237\n",
      "   macro avg       0.55      0.58      0.56      2237\n",
      "weighted avg       0.74      0.70      0.71      2237\n",
      "\n",
      "accuracy = 0.70\n",
      "Epoch: 06 | Epoch Time: 5m 45s\n",
      "\tTrain Loss: 0.329 | Train Acc: 89.85%\n",
      "\t Val. Loss: 1.508 |  Val. Acc: 69.65%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.30      0.29       237\n",
      "           1       0.85      0.77      0.81      1370\n",
      "           2       0.29      0.42      0.34       247\n",
      "           3       0.77      0.78      0.78       383\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2237\n",
      "   macro avg       0.55      0.57      0.55      2237\n",
      "weighted avg       0.71      0.68      0.70      2237\n",
      "\n",
      "accuracy = 0.68\n",
      "Epoch: 07 | Epoch Time: 5m 44s\n",
      "\tTrain Loss: 0.255 | Train Acc: 92.77%\n",
      "\t Val. Loss: 1.809 |  Val. Acc: 68.35%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.30      0.29       233\n",
      "           1       0.84      0.77      0.81      1359\n",
      "           2       0.28      0.42      0.34       242\n",
      "           3       0.80      0.76      0.78       403\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2237\n",
      "   macro avg       0.55      0.56      0.55      2237\n",
      "weighted avg       0.72      0.68      0.70      2237\n",
      "\n",
      "accuracy = 0.68\n",
      "Epoch: 08 | Epoch Time: 5m 43s\n",
      "\tTrain Loss: 0.195 | Train Acc: 95.04%\n",
      "\t Val. Loss: 2.006 |  Val. Acc: 68.40%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.28      0.30       274\n",
      "           1       0.82      0.78      0.80      1310\n",
      "           2       0.32      0.39      0.35       292\n",
      "           3       0.75      0.80      0.77       361\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      2237\n",
      "   macro avg       0.55      0.56      0.55      2237\n",
      "weighted avg       0.68      0.67      0.68      2237\n",
      "\n",
      "accuracy = 0.67\n",
      "Epoch: 09 | Epoch Time: 5m 42s\n",
      "\tTrain Loss: 0.125 | Train Acc: 96.99%\n",
      "\t Val. Loss: 2.121 |  Val. Acc: 67.14%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.28      0.29       276\n",
      "           1       0.83      0.78      0.80      1329\n",
      "           2       0.30      0.40      0.34       269\n",
      "           3       0.75      0.80      0.77       363\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      2237\n",
      "   macro avg       0.55      0.56      0.55      2237\n",
      "weighted avg       0.69      0.67      0.68      2237\n",
      "\n",
      "accuracy = 0.67\n",
      "Epoch: 10 | Epoch Time: 5m 41s\n",
      "\tTrain Loss: 0.089 | Train Acc: 97.74%\n",
      "\t Val. Loss: 2.197 |  Val. Acc: 67.37%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "best_macro_f1 = float('0')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss,train_acc = train()\n",
    "    valid_loss,valid_acc,macro_f1 = evaluate()\n",
    "    \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model,'model_hindi_task_b.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "model = torch.load('model_hindi_task_b.pt')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.57      0.31        21\n",
      "           1       0.85      0.85      0.85       492\n",
      "           2       0.54      0.34      0.42       138\n",
      "           3       0.30      0.67      0.41        12\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       663\n",
      "   macro avg       0.47      0.61      0.50       663\n",
      "weighted avg       0.75      0.73      0.73       663\n",
      "\n",
      "accuracy = 0.73\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test():\n",
    "    model.eval()\n",
    "        \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    number_of_eval_steps= 0\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    all_sentence_id=[]\n",
    "\n",
    "    for batch in test_data_loader:\n",
    "        b_sentence_id = batch[0].to(device)\n",
    "        b_input_ids = batch[1].to(device)\n",
    "        b_input_mask = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        sent_ids = b_sentence_id.to('cpu').numpy()\n",
    "        all_sentence_id.extend(sent_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                attention_mask= b_input_mask,\n",
    "                                labels = b_labels.long())\n",
    "        total_eval_loss+=loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "\n",
    "        pred,true = predictions_labels(logits,label_ids)\n",
    "        \n",
    "        all_pred_labels.extend(pred)\n",
    "        \n",
    "        all_true_labels.extend(true)\n",
    "\n",
    "    print(classification_report(all_pred_labels,all_true_labels))\n",
    "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
    "    \n",
    "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
    "\n",
    "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    return avg_val_loss,avg_val_accuracy,all_sentence_id,all_pred_labels\n",
    "\n",
    "valid_loss,valid_acc,all_sentence_id,all_pred_labels = evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = list(le.inverse_transform(all_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "dict_index_y_value = {}\n",
    "\n",
    "for index,y_value in zip(all_sentence_id,pred_labels):\n",
    "    dict_index_y_value[index] = y_value\n",
    "\n",
    "od = OrderedDict(sorted(dict_index_y_value.items()))\n",
    "\n",
    "sorted_y_predicts = []\n",
    "for k,v in od.items():\n",
    "    sorted_y_predicts.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet_id = list(df_test['tweet_id'].values)\n",
    "test_ID = list(df_test['ID'].values)\n",
    "\n",
    "test_final = pd.DataFrame(zip(test_tweet_id,sorted_y_predicts,test_ID),columns=['tweet_id',task,'ID'])\n",
    "\n",
    "if task == 'task1':\n",
    "    test_final.to_csv(\"submission_HI_A.csv\",index=False)\n",
    "\n",
    "if task == 'task2':\n",
    "    test_final.to_csv(\"submission_HI_B.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3_env)",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
