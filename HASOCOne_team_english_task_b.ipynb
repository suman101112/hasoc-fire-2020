{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3708\n",
      "                                                                  text task1  \\\n",
      "tweet_id                                                                       \n",
      "1123757263427186690  hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...   HOF   \n",
      "1123733301397733380  RT @airjunebug: When you're from the Bay but y...   HOF   \n",
      "1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   NOT   \n",
      "1126951188170199049  RT @SheLoveTimothy: He ainâ€™t on drugs he just ...   HOF   \n",
      "1126863510447710208  RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...   NOT   \n",
      "\n",
      "                    task2                  ID  \n",
      "tweet_id                                       \n",
      "1123757263427186690  PRFN  hasoc_2020_en_2574  \n",
      "1123733301397733380  PRFN  hasoc_2020_en_3627  \n",
      "1123734094108659712  NONE  hasoc_2020_en_3108  \n",
      "1126951188170199049  PRFN  hasoc_2020_en_3986  \n",
      "1126863510447710208  NONE  hasoc_2020_en_5152  \n",
      "{0, 1, 2, 3}\n",
      "10713 10713 814 814\n",
      "                tweet_id                                               text  \\\n",
      "0    1130081762154090497  RT @delmiyaa: Samini resetting the show and mo...   \n",
      "1    1130048316807491584           @Swxnsea how do you know that heâ€™s left?   \n",
      "2    1123657766143504386  Tried to get Divock Origi on a free seeing as ...   \n",
      "3    1126782963042013186  RT @nutclusteruwu: that....is yalls stupid whi...   \n",
      "4    1130159113529434113  &amp; IT DID. But a bitch got big girls things...   \n",
      "..                   ...                                                ...   \n",
      "809  1127061607433900032  RT @nytmike: At least twice in the past month,...   \n",
      "810  1123685826074951681  @ThreeDailey Omg heâ€™s so gross! This just turn...   \n",
      "811  1126882552587927552  RT @RFERL: Two alleged #GRU agents and 12 othe...   \n",
      "812  1130294488859996160  RT @ShefVaidya: In Modi 2.0, I do hope the two...   \n",
      "813  1130111650780991493  RT @SSTweeps: #UAE BO #May16-18 wknd:\\n\\n#DeDe...   \n",
      "\n",
      "    task1 task2                  ID  \n",
      "0     NOT  NONE  hasoc_2020_en_2713  \n",
      "1     HOF  NONE  hasoc_2020_en_3874  \n",
      "2     NOT  NONE   hasoc_2020_en_281  \n",
      "3     HOF  PRFN  hasoc_2020_en_2026  \n",
      "4     HOF  PRFN  hasoc_2020_en_4023  \n",
      "..    ...   ...                 ...  \n",
      "809   NOT  NONE  hasoc_2020_en_1212  \n",
      "810   HOF  OFFN  hasoc_2020_en_3435  \n",
      "811   NOT  NONE  hasoc_2020_en_3987  \n",
      "812   NOT  NONE  hasoc_2020_en_1176  \n",
      "813   HOF  NONE  hasoc_2020_en_1937  \n",
      "\n",
      "[814 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#2020 train and test files\n",
    "\n",
    "file = \"hasoc_2020_en_train_new_b.xlsx\"\n",
    "file_test = \"english_test_1509.csv\"\n",
    "\n",
    "\n",
    "df_train = pd.read_excel(file,index_col=0)\n",
    "\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "df_test = pd.read_csv(file_test)\n",
    "\n",
    "task = 'task2'\n",
    "task_2019 = 'task_2'\n",
    "\n",
    "#2019 datasets also\n",
    "\n",
    "file_2019_1 = pd.read_csv(\"2019/english_dataset/english_dataset/hasoc2019_en_test-2919.tsv\",sep='\\t')\n",
    "file_2019_2 = pd.read_csv(\"2019/english_dataset/english_dataset/english_dataset.tsv\",sep=\"\\t\")\n",
    "\n",
    "\n",
    "sentences_2019_1 = list(file_2019_1['text'].values)\n",
    "sentences_2019_2 = list(file_2019_2['text'].values)\n",
    "\n",
    "labels_2019_1 = list(file_2019_1[task_2019].values)\n",
    "labels_2019_2 = list(file_2019_2[task_2019].values)\n",
    "\n",
    "print(len(df_train))\n",
    "print(df_train.head())\n",
    "\n",
    "total_sentences = list(df_train['text'].values)\n",
    "total_labels = list(df_train[task].values)\n",
    "\n",
    "total_sentences.extend(sentences_2019_1)\n",
    "total_sentences.extend(sentences_2019_2)\n",
    "\n",
    "total_labels.extend(labels_2019_1)\n",
    "total_labels.extend(labels_2019_2)\n",
    "\n",
    "test_sentences = list(df_test['text'].values)\n",
    "test_labels = list(df_test[task].values)\n",
    "\n",
    "def clean_text(sentences):\n",
    "    for index,line in enumerate(sentences):\n",
    "        if \"\\n\" in line:\n",
    "            sentences[index] = line.replace(\"\\n\",\"\")\n",
    "    return sentences\n",
    "        \n",
    "total_sentences = clean_text(total_sentences)\n",
    "test_sentences = clean_text(test_sentences)\n",
    "\n",
    "def clean_labels(labels):\n",
    "    new_list= []\n",
    "    for value in labels:\n",
    "        new_list.append(value.strip())\n",
    "    return new_list\n",
    "\n",
    "total_labels = clean_labels(total_labels)\n",
    "test_labels = clean_labels(test_labels)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(total_labels)\n",
    "encoded_labels = le.transform(total_labels)\n",
    "encoded_test_labels = le.transform(test_labels)\n",
    "print(set(encoded_labels))\n",
    "\n",
    "print(len(total_sentences),len(encoded_labels),len(test_sentences),len(encoded_test_labels))\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length is:  399\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 0\n",
    "for sentence in total_sentences:\n",
    "    #print(sentence)\n",
    "    length = len(tokenizer.tokenize(sentence))\n",
    "    if length > max_length:\n",
    "        max_length  = length\n",
    "print(\"max token length is: \",max_length)\n",
    "# max token length obtained is 50\n",
    "# bert tokens are limited to 514 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚, Iâ€™m tryna make u my la sweety , fuck ah bro\n",
      "Token IDs: tensor([  101,  5223, 19181,  3801,  2718,  6289,  9152, 23033,  2007, 16215,\n",
      "         2102, 22953,   100,  1010,  1045,  1521,  1049,  3046,  2532,  2191,\n",
      "         1057,  2026,  2474,  4086,  2100,  1010,  6616,  6289, 22953,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "def encoder_generator(sentences,labels):\n",
    "    \n",
    "    sent_index = []\n",
    "    input_ids = []\n",
    "    attention_masks =[]\n",
    "\n",
    "    for index,sent in enumerate(sentences):\n",
    "        \n",
    "        sent_index.append(index)\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=128,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             truncation = True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids,dim=0)\n",
    "    attention_masks = torch.cat(attention_masks,dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    sent_index = torch.tensor(sent_index)\n",
    "\n",
    "    return sent_index,input_ids,attention_masks,labels\n",
    "\n",
    "sent_index,input_ids,attention_masks,encoded_label_tensors = encoder_generator(total_sentences,encoded_labels)\n",
    "test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors = encoder_generator(test_sentences,encoded_test_labels)\n",
    "print('Original: ', total_sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "#print(encoded_label_tensors)\n",
    "#print(encoded_test_label_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data samples is 8034\n",
      "valid data samples is 2679\n",
      "test data samples is 814\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset,random_split\n",
    "\n",
    "dataset = TensorDataset(input_ids,attention_masks,encoded_label_tensors)\n",
    "test_dataset = TensorDataset(test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors)\n",
    "\n",
    "train_size = int(0.75*len(dataset))\n",
    "\n",
    "val_size = len(dataset)-train_size\n",
    "\n",
    "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\n",
    "\n",
    "print('train data samples is {}'.format(len(train_dataset)))\n",
    "print(\"valid data samples is {}\".format(len(val_dataset)))\n",
    "print(\"test data samples is {}\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "bs=8\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,\n",
    "                              sampler=RandomSampler(train_dataset),\n",
    "                              batch_size=bs)\n",
    "valid_data_loader = DataLoader(val_dataset,\n",
    "                              sampler=SequentialSampler(val_dataset),\n",
    "                              batch_size=bs)\n",
    "test_data_loader = DataLoader(test_dataset,\n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                     num_labels=len(le.classes_),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False,\n",
    "                                                     )\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs=10\n",
    "total_steps = len(train_data_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=0,\n",
    "                                           num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predictions_labels(preds,labels):\n",
    "    pred = np.argmax(preds,axis=1).flatten()\n",
    "    label = labels.flatten()\n",
    "    return pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "def predictions_labels(preds,labels):\n",
    "    pred = np.argmax(preds,axis=1).flatten()\n",
    "    label = labels.flatten()\n",
    "    return pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    \n",
    "    model.train() # set model in train mode for batchnorm and dropout layers in bert model\n",
    "    \n",
    "    for step,batch in enumerate(train_data_loader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "            \n",
    "        loss,logits = model(b_input_ids,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels.long())\n",
    "            \n",
    "        total_train_loss+=loss.item()\n",
    "        total_train_acc+=categorical_accuracy(logits,b_labels).item()\n",
    "            \n",
    "        loss.backward()\n",
    "            \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            \n",
    "        optimizer.step()\n",
    "            \n",
    "        scheduler.step() #go ahead and update the learning rate\n",
    "            \n",
    "    avg_train_loss = total_train_loss/len(train_data_loader)\n",
    "    avg_train_acc = total_train_acc/len(train_data_loader)\n",
    "    \n",
    "    return avg_train_loss,avg_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "        \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    number_of_eval_steps= 0\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    for batch in valid_data_loader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                attention_mask= b_input_mask,\n",
    "                                labels = b_labels.long())\n",
    "        total_eval_loss+=loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred,true = predictions_labels(logits,label_ids)\n",
    "        \n",
    "        all_pred_labels.extend(pred)\n",
    "        all_true_labels.extend(true)\n",
    "\n",
    "    print(classification_report(all_pred_labels,all_true_labels))\n",
    "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
    "    macro_f1_score = f1_score(all_pred_labels,all_true_labels,average='macro')\n",
    "    \n",
    "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
    "\n",
    "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    return avg_val_loss,avg_val_accuracy,macro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.95      0.73      0.83      2009\n",
      "           2       0.13      0.34      0.19        77\n",
      "           3       0.81      0.77      0.79       591\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2679\n",
      "   macro avg       0.47      0.46      0.45      2679\n",
      "weighted avg       0.90      0.73      0.80      2679\n",
      "\n",
      "accuracy = 0.73\n",
      "Epoch: 01 | Epoch Time: 3m 47s\n",
      "\tTrain Loss: 0.822 | Train Acc: 70.22%\n",
      "\t Val. Loss: 0.790 |  Val. Acc: 72.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.33      0.07        42\n",
      "           1       0.93      0.74      0.83      1934\n",
      "           2       0.22      0.33      0.27       132\n",
      "           3       0.77      0.77      0.77       571\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      2679\n",
      "   macro avg       0.49      0.54      0.48      2679\n",
      "weighted avg       0.85      0.72      0.78      2679\n",
      "\n",
      "accuracy = 0.72\n",
      "Epoch: 02 | Epoch Time: 3m 47s\n",
      "\tTrain Loss: 0.678 | Train Acc: 74.93%\n",
      "\t Val. Loss: 0.784 |  Val. Acc: 72.08%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.36      0.34       332\n",
      "           1       0.84      0.77      0.81      1684\n",
      "           2       0.17      0.33      0.22       104\n",
      "           3       0.76      0.77      0.76       559\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      2679\n",
      "   macro avg       0.52      0.56      0.53      2679\n",
      "weighted avg       0.74      0.70      0.72      2679\n",
      "\n",
      "accuracy = 0.70\n",
      "Epoch: 03 | Epoch Time: 3m 51s\n",
      "\tTrain Loss: 0.531 | Train Acc: 80.49%\n",
      "\t Val. Loss: 0.840 |  Val. Acc: 70.29%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.32      0.30       348\n",
      "           1       0.83      0.78      0.80      1653\n",
      "           2       0.21      0.32      0.25       133\n",
      "           3       0.74      0.77      0.76       545\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2679\n",
      "   macro avg       0.52      0.54      0.53      2679\n",
      "weighted avg       0.71      0.69      0.70      2679\n",
      "\n",
      "accuracy = 0.69\n",
      "Epoch: 04 | Epoch Time: 3m 46s\n",
      "\tTrain Loss: 0.372 | Train Acc: 87.39%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 69.20%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.30      0.30       371\n",
      "           1       0.81      0.79      0.80      1581\n",
      "           2       0.21      0.28      0.24       148\n",
      "           3       0.75      0.74      0.74       579\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2679\n",
      "   macro avg       0.52      0.53      0.52      2679\n",
      "weighted avg       0.69      0.68      0.69      2679\n",
      "\n",
      "accuracy = 0.68\n",
      "Epoch: 05 | Epoch Time: 3m 41s\n",
      "\tTrain Loss: 0.246 | Train Acc: 92.72%\n",
      "\t Val. Loss: 1.502 |  Val. Acc: 68.01%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.29      0.34       513\n",
      "           1       0.75      0.81      0.78      1432\n",
      "           2       0.20      0.30      0.24       128\n",
      "           3       0.76      0.71      0.74       606\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      2679\n",
      "   macro avg       0.53      0.53      0.52      2679\n",
      "weighted avg       0.66      0.66      0.66      2679\n",
      "\n",
      "accuracy = 0.66\n",
      "Epoch: 06 | Epoch Time: 3m 34s\n",
      "\tTrain Loss: 0.165 | Train Acc: 95.58%\n",
      "\t Val. Loss: 1.897 |  Val. Acc: 66.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.30      0.29       364\n",
      "           1       0.82      0.79      0.80      1587\n",
      "           2       0.23      0.31      0.27       151\n",
      "           3       0.75      0.73      0.74       577\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2679\n",
      "   macro avg       0.52      0.53      0.53      2679\n",
      "weighted avg       0.70      0.69      0.69      2679\n",
      "\n",
      "accuracy = 0.69\n",
      "Epoch: 07 | Epoch Time: 3m 35s\n",
      "\tTrain Loss: 0.109 | Train Acc: 97.30%\n",
      "\t Val. Loss: 2.116 |  Val. Acc: 68.53%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.31      0.29       327\n",
      "           1       0.83      0.78      0.81      1637\n",
      "           2       0.21      0.28      0.24       149\n",
      "           3       0.74      0.74      0.74       566\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2679\n",
      "   macro avg       0.51      0.53      0.52      2679\n",
      "weighted avg       0.71      0.69      0.70      2679\n",
      "\n",
      "accuracy = 0.69\n",
      "Epoch: 08 | Epoch Time: 3m 34s\n",
      "\tTrain Loss: 0.071 | Train Acc: 98.25%\n",
      "\t Val. Loss: 2.181 |  Val. Acc: 68.83%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.28      0.29       402\n",
      "           1       0.79      0.79      0.79      1545\n",
      "           2       0.22      0.28      0.25       158\n",
      "           3       0.73      0.72      0.73       574\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      2679\n",
      "   macro avg       0.51      0.52      0.51      2679\n",
      "weighted avg       0.67      0.67      0.67      2679\n",
      "\n",
      "accuracy = 0.67\n",
      "Epoch: 09 | Epoch Time: 3m 33s\n",
      "\tTrain Loss: 0.054 | Train Acc: 98.79%\n",
      "\t Val. Loss: 2.374 |  Val. Acc: 66.89%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.29      0.29       395\n",
      "           1       0.81      0.79      0.80      1566\n",
      "           2       0.24      0.29      0.26       165\n",
      "           3       0.73      0.74      0.73       553\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      2679\n",
      "   macro avg       0.52      0.53      0.52      2679\n",
      "weighted avg       0.68      0.68      0.68      2679\n",
      "\n",
      "accuracy = 0.68\n",
      "Epoch: 10 | Epoch Time: 3m 34s\n",
      "\tTrain Loss: 0.034 | Train Acc: 99.18%\n",
      "\t Val. Loss: 2.369 |  Val. Acc: 67.56%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "best_macro_f1 = float('0')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss,train_acc = train()\n",
    "    valid_loss,valid_acc,macro_f1 = evaluate()\n",
    "    \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model,'model_english_task_b.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "model = torch.load('model_english_task_b.pt')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.15      0.13        20\n",
      "           1       0.93      0.88      0.90       439\n",
      "           2       0.21      0.57      0.30        30\n",
      "           3       0.88      0.80      0.84       325\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       814\n",
      "   macro avg       0.54      0.60      0.54       814\n",
      "weighted avg       0.87      0.82      0.84       814\n",
      "\n",
      "accuracy = 0.82\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test():\n",
    "    model.eval()\n",
    "        \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    number_of_eval_steps= 0\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    all_sentence_id=[]\n",
    "\n",
    "    for batch in test_data_loader:\n",
    "        b_sentence_id = batch[0].to(device)\n",
    "        b_input_ids = batch[1].to(device)\n",
    "        b_input_mask = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        sent_ids = b_sentence_id.to('cpu').numpy()\n",
    "        all_sentence_id.extend(sent_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                attention_mask= b_input_mask,\n",
    "                                labels = b_labels.long())\n",
    "        total_eval_loss+=loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "\n",
    "        pred,true = predictions_labels(logits,label_ids)\n",
    "        \n",
    "        all_pred_labels.extend(pred)\n",
    "        \n",
    "        all_true_labels.extend(true)\n",
    "\n",
    "    print(classification_report(all_pred_labels,all_true_labels))\n",
    "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
    "    \n",
    "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
    "\n",
    "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "    return avg_val_loss,avg_val_accuracy,all_sentence_id,all_pred_labels\n",
    "\n",
    "valid_loss,valid_acc,all_sentence_id,all_pred_labels = evaluate_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = list(le.inverse_transform(all_pred_labels))\n",
    "from collections import OrderedDict\n",
    "dict_index_y_value = {}\n",
    "\n",
    "for index,y_value in zip(all_sentence_id,pred_labels):\n",
    "    dict_index_y_value[index] = y_value\n",
    "\n",
    "od = OrderedDict(sorted(dict_index_y_value.items()))\n",
    "\n",
    "sorted_y_predicts = []\n",
    "for k,v in od.items():\n",
    "    sorted_y_predicts.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet_id = list(df_test['tweet_id'].values)\n",
    "test_ID = list(df_test['ID'].values)\n",
    "\n",
    "test_final = pd.DataFrame(zip(test_tweet_id,sorted_y_predicts,test_ID),columns=['tweet_id',task,'ID'])\n",
    "\n",
    "if task == 'task1':\n",
    "    test_final.to_csv(\"submission_EN_A.csv\",index=False)\n",
    "\n",
    "if task == 'task2':\n",
    "    test_final.to_csv(\"submission_EN_B.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3_env)",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
